{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to slip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 初始化 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n",
    "sep_id = tokenizer.sep_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "special_tks = list()\n",
    "special_tks.append('#@#') # code separator\n",
    "special_tks.append('<NL>') # code separator\n",
    "# special_tks.append('<None>') # code separator\n",
    "special_tks.append('[CLS]') # [CLS] 開頭\n",
    "special_tks.append('[PAD]') # [PAD] 中間\n",
    "special_tks.append('[SEP]') # [SEP] 結束\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tks})\n",
    "\n",
    "max_length = 0\n",
    "longest_data = None\n",
    "data_set = []\n",
    "data_set2 = []\n",
    "# big_label_list = []\n",
    "\n",
    "too_longdata = []\n",
    "kmuh_reports_dict = dict()\n",
    "\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\data'\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\data_old'\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\data_old_new'\n",
    "lable_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\answer.txt'\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取answer.txt中的标签数据\n",
    "with open(lable_filepath, 'r', encoding='utf-8') as label_file:\n",
    "    labels = label_file.readlines()\n",
    "\n",
    "txt_files = glob.glob(os.path.join(data_filepath, '*.txt'))\n",
    "\n",
    "# for txt_file in txt_files:\n",
    "for txt_file in tqdm(txt_files, desc='Processing files', unit='file'):\n",
    "    # print(txt_file)\n",
    "    with open(txt_file, 'r', encoding='utf-8') as data_file:\n",
    "        data = data_file.read()\n",
    "    data = clean_text(data)\n",
    "\n",
    "    slip_num = 15\n",
    "    split_size = len(data) // slip_num #+ ((len(data) // (slip_num-1) - len(data) // slip_num)//int(slip_num*2.5))\n",
    "\n",
    "    gc = 0\n",
    "\n",
    "    splits = []\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < len(data):\n",
    "        if gc == slip_num:\n",
    "            print('warring1=', split_size)\n",
    "\n",
    "        end_index = start_index + split_size\n",
    "\n",
    "        # Check if the next character after the split is a space\n",
    "        if end_index < len(data) and data[end_index] != ' ':\n",
    "            # Move the end index forward until a space is found\n",
    "            while end_index < len(data) and data[end_index] != ' ':\n",
    "                end_index += 1\n",
    "\n",
    "        splits.append(data[start_index:end_index])\n",
    "        gc += 1\n",
    "        start_index = end_index\n",
    "\n",
    "    # if gc == (slip_num-2):\n",
    "    #     print('warring2=', split_size)\n",
    "\n",
    "        # splits = []\n",
    "        # splits.append(data)\n",
    "    label_list = []\n",
    "    for label in labels:\n",
    "        label_parts = label.split('\\t')\n",
    "        if label_parts[0] == os.path.basename(txt_file).split('.')[0]:\n",
    "            label_list.append(label)\n",
    "            # big_label_list.append(label)\n",
    "    for data_slipt in splits:\n",
    "        matched_label = ''\n",
    "        data_with_label = ''\n",
    "        data_with_label2 = ''\n",
    "        for j in label_list:\n",
    "            label_parts = j.split('\\t')\n",
    "            # print( label_parts[4] )\n",
    "            if label_parts[4].strip() in data_slipt.strip():\n",
    "                try:\n",
    "                    matched_label += label_parts[1].strip() + \"\\t\" + label_parts[4].strip() + \"\\t\" + label_parts[5].strip() + '#@#'\n",
    "                except:\n",
    "                    matched_label += label_parts[1].strip() + \"\\t\" + label_parts[4].strip() + '#@#'\n",
    "        matched_label = re.sub(r'\\t', '<T>', matched_label)\n",
    "        matched_label = re.sub(r'\\s+', '<NL>', matched_label)\n",
    "        matched_label = re.sub('<T>', r'\\t', matched_label)\n",
    "        data_slipt = re.sub(r'\\t', '<T>', data_slipt.strip())\n",
    "        data_slipt = re.sub(r'\\s+', '<NL>', data_slipt)\n",
    "        data_slipt = re.sub('<T>', r'\\t', data_slipt)\n",
    "        data_with_label = data_slipt + tokenizer.sep_token + matched_label.replace('\\n','').rstrip('#@#')\n",
    "        data_with_label2 = data_slipt + \" => \" + matched_label.replace('\\n','').rstrip('#@#')\n",
    "\n",
    "\n",
    "    #     if len(data_with_label) > 1024:\n",
    "            # print(txt_file)\n",
    "    #         continue\n",
    "        # print('data_with_label=\\n', data_with_label)\n",
    "        data_set.append(data_with_label)\n",
    "        data_set2.append(data_with_label2)\n",
    "\n",
    "data_set = data_set + data_set + data_set + data_set + data_set + data_set\n",
    "data_set = data_set + data_set\n",
    "data_with_label3 = data_set\n",
    "\n",
    "train_list = list() # 转换为列表\n",
    "max_length = 0\n",
    "for data in data_set:\n",
    "    if len(data.split(\"<NL>\")) > max_length:\n",
    "        max_length = len(data.split(\"<NL>\"))\n",
    "    input_ids = [cls_id] # 以 [CLS] 開頭\n",
    "    input_ids += tokenizer.encode(data, add_special_tokens=False)\n",
    "    input_ids.append(sep_id) # 加上 [SEP], 表示報告內容結束\n",
    "    train_list.append(input_ids)\n",
    "    \n",
    "print(\"最長的數量是:\", max_length)\n",
    "\n",
    "import random\n",
    "# 隨機排序資料集\n",
    "random.shuffle(train_list)\n",
    "# 計算切分的索引\n",
    "train_size = int(0.98 * len(train_list))\n",
    "# 切分訓練集和測試集\n",
    "train_set = train_list[:train_size]\n",
    "test_set = train_list[train_size:]\n",
    "\n",
    "# 保存训练集到train.pkl\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\train.pkl', \"wb\") as f:\n",
    "    pickle.dump(train_set, f)\n",
    "\n",
    "# 保存测试集到test.pkl\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\test.pkl', \"wb\") as f:\n",
    "    pickle.dump(test_set, f)\n",
    "train_list = list() # 转换为列表\n",
    "\n",
    "# 写入数据到文件\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\train.txt', \"w\", encoding='utf-8') as file:\n",
    "    for j in data_with_label3:\n",
    "        file.write(f\"{j}\\n\")\n",
    "\n",
    "print(\"訓練集数量:\", len(train_set))\n",
    "print(\"測試集数量:\", len(test_set))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vaild - pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\t+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# 初始化 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n",
    "sep_id = tokenizer.sep_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "cls_id = tokenizer.cls_token_id\n",
    "special_tks = list()\n",
    "special_tks.append('#@#') # code separator\n",
    "special_tks.append('<NL>') # code separator\n",
    "# special_tks.append('<None>') # code separator\n",
    "special_tks.append('[CLS]') # [CLS] 開頭\n",
    "special_tks.append('[PAD]') # [PAD] 中間\n",
    "special_tks.append('[SEP]') # [SEP] 結束\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tks})\n",
    "\n",
    "max_length = 0\n",
    "longest_data = None\n",
    "data_set = []\n",
    "data_set2 = []\n",
    "\n",
    "too_longdata = []\n",
    "kmuh_reports_dict = dict()\n",
    "\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\Second_Phase_Dataset\\Second_Phase_Dataset\\Second_Phase_Text_Dataset'\n",
    "lable_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\Second_Phase_Dataset\\Second_Phase_Dataset\\answer.txt'\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\First_Phase_ReleaseCorrection\\First_Phase_Release(Correction)\\Validation_Release'\n",
    "lable_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\First_Phase_ReleaseCorrection\\answer.txt'\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取answer.txt中的标签数据\n",
    "with open(lable_filepath, 'r', encoding='utf-8') as label_file:\n",
    "    labels = label_file.readlines()\n",
    "\n",
    "txt_files = glob.glob(os.path.join(data_filepath, '*.txt'))\n",
    "\n",
    "for txt_file in tqdm(txt_files, desc='Processing files', unit='file'):\n",
    "    with open(txt_file, 'r', encoding='utf-8') as data_file:\n",
    "        data = data_file.read()\n",
    "    data = clean_text(data)\n",
    "\n",
    "    flag = False\n",
    "    flag2 = False\n",
    "\n",
    "    slip_num = 15\n",
    "    split_size = len(data) // slip_num #(slip_num + ((len(data) // (slip_num-2) - len(data) // (slip_num-1) )//int(slip_num*2.5)))\n",
    "    gc = 0\n",
    "\n",
    "    splits = []\n",
    "    start_index = 0\n",
    "    \n",
    "    while start_index < len(data):\n",
    "        if gc == slip_num:\n",
    "            print('warring1=', split_size)\n",
    "        \n",
    "        end_index = start_index + split_size\n",
    "\n",
    "        # Check if the next character after the split is a space\n",
    "        if end_index < len(data) and data[end_index] != ' ':\n",
    "            # Move the end index forward until a space is found\n",
    "            while end_index < len(data) and data[end_index] != ' ':\n",
    "                end_index += 1\n",
    "\n",
    "        splits.append(data[start_index:end_index])\n",
    "        gc += 1\n",
    "\n",
    "        start_index = end_index\n",
    "    if gc == (slip_num-2):\n",
    "        print('warring2=', split_size)\n",
    "    # if split_size > 400:\n",
    "    #     print('a>400',split_size)\n",
    "    if len(splits) != slip_num:\n",
    "        print('some error', len(splits), \" split_size=\",split_size,)\n",
    "     \n",
    "    for data_slipt in splits:\n",
    "        data_with_label2 = ''\n",
    "        data_slipt = re.sub(r'\\t', '<T>', data_slipt.strip())\n",
    "        data_slipt = re.sub(r'\\s+', '<NL>', data_slipt)\n",
    "        # data_slipt = re.sub('<NL> <NL>', '<NL>', data_slipt)\n",
    "        data_slipt = re.sub('<T>', r'\\t', data_slipt)\n",
    "        # data_slipt = clean_text(data_slipt)\n",
    "        data_with_label2 = data_slipt\n",
    "        data_set2.append(data_with_label2)\n",
    "train_list = list() # 转换为列表\n",
    "\n",
    "max_length = 0\n",
    "for data in data_set2:\n",
    "    if len(data.split(\"<NL>\")) > max_length:\n",
    "        max_length = len(data.split(\"<NL>\"))\n",
    "\n",
    "    input_ids = [cls_id] # 以 [CLS] 開頭\n",
    "    input_ids += tokenizer.encode(data, add_special_tokens=False)\n",
    "    input_ids.append(sep_id) # 加上 [SEP], 表示報告內容結束\n",
    "    train_list.append(input_ids)\n",
    "print(\"最長的數量是:\", max_length)\n",
    "\n",
    "# 保存测试集到test.pkl\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\test2.pkl', \"wb\") as f:\n",
    "    pickle.dump(train_list, f)\n",
    "print(\"訓練集数量:\", len(train_list))\n",
    "\n",
    "train_list = list() # 转换为列表\n",
    "\n",
    "for data in data_set2:\n",
    "    train_list.append(data)\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\test.txt', \"w\", encoding='utf-8') as file:\n",
    "    for j in train_list:\n",
    "        file.write(f\"{j}\\n\")\n",
    "\n",
    "print(\"測試集数量:\", len(train_list))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find answer position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\answer.txt\"\n",
    "file_path = r\"C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\First_Phase_ReleaseCorrection\\answer.txt\"\n",
    "element_dict = {}\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split(\"\\t\")\n",
    "        if len(elements) >= 2:\n",
    "            element = elements[1]\n",
    "            element_dict[element] = True\n",
    "\n",
    "element_list = list(element_dict.keys())\n",
    "print(element_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = re.sub(',', ' ', text)\n",
    "    # text = re.sub(r'\\t+', '\\t', text)\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    return text\n",
    "\n",
    "max_length = 0\n",
    "longest_data = None\n",
    "data_set = []\n",
    "data_set2 = []\n",
    "\n",
    "too_longdata = []\n",
    "kmuh_reports_dict = dict()\n",
    "\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\Second_Phase_Dataset\\Second_Phase_Dataset\\Second_Phase_Text_Dataset'\n",
    "data_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\First_Phase_ReleaseCorrection\\First_Phase_Release(Correction)\\Validation_Release'\n",
    "lable_filepath = r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\predict.txt'\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# 读取answer.txt中的标签数据\n",
    "with open(lable_filepath, 'r', encoding='utf-8') as label_file:\n",
    "    labels = label_file.readlines()\n",
    "# 移除空行\n",
    "labels = [label for label in labels if label.strip()]\n",
    "\n",
    "txt_files = glob.glob(os.path.join(data_filepath, '*.txt'))\n",
    "\n",
    "for txt_file in tqdm(txt_files, desc='Processing files', unit='file'):\n",
    "    # print(txt_file)\n",
    "    with open(txt_file, 'r', encoding='utf-8') as data_file:\n",
    "        data = data_file.read()\n",
    "    data = clean_text(data)\n",
    "    txt_file = txt_file.replace('C:\\\\Users\\\\Administrator\\\\Desktop\\\\old gpt2\\\\aicup\\\\First_Phase_ReleaseCorrection\\\\First_Phase_Release(Correction)\\\\Validation_Release\\\\','').replace('.txt', '')\n",
    "    # print(txt_file)\n",
    "    label_count = {}\n",
    "    # label_temporary = []\n",
    "    for label in labels:\n",
    "        word = ''\n",
    "        if label.strip() == '=====================':\n",
    "            break\n",
    "        try:\n",
    "            label_parts = label.split('\\t')\n",
    "            word = label_parts[1].strip()\n",
    "            word = str(word) #.strip())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            label_count[word] += 1\n",
    "        except:\n",
    "            label_count[word] = 0\n",
    "    for label in labels:\n",
    "        # print(label)\n",
    "        matches = []\n",
    "        # start_index = 0\n",
    "        # end_index = 0\n",
    "        word = ''\n",
    "        if label.strip() == '=====================':\n",
    "            break\n",
    "        try:\n",
    "            label_parts = label.split('\\t')\n",
    "            word = label_parts[1].strip()\n",
    "            word = str(word) #.strip())\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            pattern = (word) + r\"\\b\"\n",
    "            matches = list(re.finditer(pattern, str(data)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        falg = False\n",
    "\n",
    "        if label_count[word] == 0:\n",
    "            start_index = matches[int(label_count[word])].start()\n",
    "            end_index = matches[int(label_count[word])].end()\n",
    "        while label_count[word] > 0:\n",
    "            try:\n",
    "                start_index = matches[label_count[word]].start()\n",
    "                end_index = matches[label_count[word]].end()\n",
    "                break  # 如果成功取得索引，跳出迴圈\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                label_count[word] = label_count[word] - 1\n",
    "\n",
    "        label_parts = label.split('\\t', 1)\n",
    "        show = txt_file + '\\t' + label_parts[0] + '\\t' + str(start_index) + '\\t' + str(end_index) + '\\t' + label_parts[1] \n",
    "        data_set.append(show)\n",
    "\n",
    "    try:\n",
    "        break_line = '====================='\n",
    "        break_index = None\n",
    "        # 寻找第一个\"==\"所在行的索引\n",
    "        for i, label in enumerate(labels):\n",
    "            if label.strip() == break_line:\n",
    "                break_index = i\n",
    "                break\n",
    "        if break_index is not None:\n",
    "            # 移除已执行过的内容\n",
    "            labels = labels[break_index + 1:]\n",
    "    except:\n",
    "        print(\"finall = \", txt_file)\n",
    "        continue\n",
    "with open(r'C:\\Users\\Administrator\\Desktop\\old gpt2\\aicup\\dataset\\answer.txt', \"w\", encoding='utf-8') as file:\n",
    "    for j in data_set:\n",
    "        file.write(f\"{j}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
